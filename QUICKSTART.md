# ğŸš€ Quick Start Guide

## Project Overview

You have successfully set up a **production-ready Arabic RAG Document Parser** with:

âœ… **Multi-format parsing** (PDF, DOCX, TXT)  
âœ… **Advanced Arabic text processing** (RTL, diacritics, normalization)  
âœ… **Intelligent chunking** (Fixed, Semantic, Auto-selector)  
âœ… **Vector storage** (ChromaDB with multilingual embeddings)  
âœ… **Metadata tracking** (SQLite database)  
âœ… **Comprehensive benchmarking** (Performance + Quality metrics)

## ğŸ“ Project Structure

```
rag2/
â”œâ”€â”€ core/                       âœ… Core processing modules
â”‚   â”œâ”€â”€ arabic_processor.py     # Arabic text processing (from extract.py)
â”‚   â”œâ”€â”€ document_parser.py      # Multi-format parsing
â”‚   â”œâ”€â”€ chunking_strategy.py    # Intelligent chunking
â”‚   â””â”€â”€ embedding_manager.py    # Embedding generation
â”œâ”€â”€ storage/                    âœ… Storage layer
â”‚   â”œâ”€â”€ vector_store.py         # ChromaDB integration
â”‚   â””â”€â”€ metadata_store.py       # SQLite metadata
â”œâ”€â”€ benchmarks/                 âœ… Benchmarking suite
â”‚   â””â”€â”€ benchmark_suite.py      # Performance metrics
â”œâ”€â”€ examples/                   âœ… Example scripts
â”‚   â”œâ”€â”€ example_usage.py        # Basic usage
â”‚   â””â”€â”€ run_benchmark.py        # Benchmark comparison
â”œâ”€â”€ data/                       âœ… Your documents
â”‚   â”œâ”€â”€ file_ar.pdf             # Test PDF
â”‚   â””â”€â”€ file.txt                # Ground truth text
â”œâ”€â”€ databases/                  âœ… Auto-created storage
â”œâ”€â”€ output/                     âœ… Benchmark results
â”œâ”€â”€ config.py                   âœ… Configuration
â”œâ”€â”€ main.py                     âœ… Main pipeline
â”œâ”€â”€ setup.py                    âœ… Setup script
â”œâ”€â”€ requirements.txt            âœ… Dependencies
â””â”€â”€ README.md                   âœ… Full documentation
```

## ğŸƒ Next Steps

### 1. Install Dependencies (if not done)

```bash
pip install -r requirements.txt
```

This will install:
- PyMuPDF & pdfplumber (PDF parsing)
- python-docx (DOCX support)
- ChromaDB (vector storage)
- sentence-transformers (embeddings)
- SQLAlchemy (metadata)
- And more...

### 2. Run Basic Example

```bash
py examples\example_usage.py
```

This will:
- Process file_ar.pdf and file.txt
- Create chunks using auto-selected strategy
- Generate embeddings
- Store in ChromaDB + SQLite
- Run sample queries

### 3. Run Comprehensive Benchmark

```bash
py examples\run_benchmark.py
```

This will:
- Test all chunking strategies
- Measure processing time & memory
- Evaluate retrieval accuracy
- Compare PDF extraction vs ground truth
- Save detailed results to output/

## ğŸ’¡ Quick Usage Examples

### Process a Single Document

```python
from main import ArabicRAGPipeline

pipeline = ArabicRAGPipeline(chunking_strategy='auto')
result = pipeline.process_document('data/file_ar.pdf')
print(f"Created {result['num_chunks']} chunks")
```

### Query the System

```python
results = pipeline.query("Ù…Ø§ Ù‡ÙŠ Ø®Ø¯Ù…Ø§Øª Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ¯ÙˆÙŠØ±ØŸ", n_results=5)

for doc in results['results']['documents'][0]:
    print(doc[:200])
```

### Run Benchmark

```python
benchmark = pipeline.run_benchmark(
    file_paths=['data/file_ar.pdf', 'data/file.txt'],
    test_queries=["Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©ØŸ"],
    ground_truth_pdf='data/file_ar.pdf',
    ground_truth_txt='data/file.txt'
)
```

## ğŸ”§ Configuration

Edit `config.py` to customize:

- **Chunking sizes**: `FIXED_CHUNK_SIZE = 512`
- **Overlap**: `FIXED_CHUNK_OVERLAP = 128`
- **Embedding model**: `EMBEDDING_MODEL = "paraphrase-multilingual-MiniLM-L12-v2"`
- **Database paths**: `CHROMA_DB_PATH`, `SQLITE_DB_PATH`

## ğŸ“Š Key Features Implemented

### From extract.py Integration
âœ… RTL text correction  
âœ… Common PDF error fixes (Ø§Ø§Ù„ â†’ Ø§Ù„, Ø§Ø£Ù„ â†’ Ø£Ù„, etc.)  
âœ… Diacritics preservation  
âœ… Arabic entity extraction  

### Intelligent Chunking
âœ… Fixed-size with overlap  
âœ… Semantic (structure-based)  
âœ… Auto-selector (analyzes document)  

### Storage
âœ… ChromaDB for vector embeddings  
âœ… SQLite for metadata tracking  
âœ… Dual-version text (retrieval + search)  

### Benchmarking
âœ… Processing time & memory  
âœ… Retrieval accuracy (Hit Rate, MRR)  
âœ… Arabic extraction quality (F1 score)  

## ğŸ¯ Testing Your Documents

1. **Add your documents** to `data/` directory
2. **Update file paths** in examples or use:

```python
pipeline = ArabicRAGPipeline()
pipeline.process_batch([
    'data/your_document.pdf',
    'data/another_document.docx'
])
```

3. **Query your content**:

```python
results = pipeline.query("your Arabic query here")
```

## ğŸ“ˆ Viewing Results

### Check Statistics

```python
stats = pipeline.get_stats()
print(f"Documents: {stats['metadata_store']['total_documents']}")
print(f"Chunks: {stats['metadata_store']['total_chunks']}")
```

### View Benchmark Results

Results are saved to:
- `output/[benchmark_name].json` - Detailed JSON results
- `databases/metadata.db` - SQLite database (queryable)

## ğŸ› ï¸ Advanced Usage

### Custom Chunking Strategy

```python
from core.chunking_strategy import SemanticChunker

chunker = SemanticChunker(
    min_chunk_size=300,
    max_chunk_size=800,
    use_embeddings=True  # Similarity-based
)

pipeline = ArabicRAGPipeline(chunking_strategy=chunker)
```

### Filter Queries by Metadata

```python
results = pipeline.query(
    "query text",
    filter_metadata={'is_arabic': True, 'file_type': 'pdf'}
)
```

## ğŸ§¹ Clean Architecture Notes

The logic from `extract.py` has been **successfully integrated** into:

- **`core/arabic_processor.py`**: 
  - `fix_rtl_extraction()` - Line 39
  - `fix_common_pdf_errors()` - Line 73
  - `normalize_for_search()` - Line 109
  - `extract_arabic_entities()` - Line 155

The original `extract.py` can now be **safely deleted** as all functionality is preserved and enhanced in the modular architecture.

## ğŸ“ Need Help?

1. Check `README.md` for full documentation
2. Review examples in `examples/`
3. Examine config in `config.py`
4. Check logs for detailed error messages

## ğŸ‰ You're Ready!

The system is fully set up and ready to process Arabic documents for RAG applications. Start with the examples and customize as needed!

---

**Built with â¤ï¸ for high-performance Arabic NLP**
